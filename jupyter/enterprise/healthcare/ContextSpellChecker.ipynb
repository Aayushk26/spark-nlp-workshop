{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Spell Checker - version 2.4.0\n",
    "\n",
    "## Example for ContextSpellChecker\n",
    "\n",
    "\n",
    "Lets imagine we have this sentence with a couple of spelling errors (in red):\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "    I <span style=\"color: red\">habe</span> four in my family: Dad, Mum and <span style=\"color: red\">sisster</span>.<br>\n",
    "</div>\n",
    "\n",
    "SparkNLP Enterprise version provides with a pretrained SpellChecker model that can fix those errors by using contextual information. This notebook provide an example of how to use this Annotator in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Prepare the environment\n",
    "\n",
    "#### Install OpenSource spark-nlp and pyspark pip packages\n",
    "As a first step we import the required python dependences including some sparknlp components.\n",
    "\n",
    "Be sure that you have the required python libraries (pyspark 2.4.4, spark-nlp 2.4.0) by running <code>pip list</code>. Check that the versions are correct.\n",
    "\n",
    "If some of them is missing you can run:\n",
    "\n",
    "<code>pip install --ignore-installed pyspark==2.4.4</code><br>\n",
    "<code>pip install --ignore-installed spark-nlp==2.4.0</code><br>\n",
    "\n",
    "The --ignore-installed parameter is to overwrite your previous pip package version if already installed.\n",
    "\n",
    "<i>*If this cell fails means you have not propertly setup the required environment. Please check the pre-requisites guideline at http://www.johnsnowlabs.com</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Licensed Sparl-NLP package\n",
    "\n",
    "We will use also some Spark-NLP enterprise functionalities contained in the spark-nlp-jsl package.\n",
    "\n",
    "You can check that spark-nlp-jsl is installed by running <code>pip install</code>. Check that version installed is 2.4.0\n",
    "\n",
    "If it is not then you need to install it by using:\n",
    "\n",
    "<code>pip install spark-nlp-jsl==2.4.0 --extra-index-url https://pypi.johnsnowlabs.com/##### --ignore-installed</code>\n",
    "\n",
    "The ##### is a secret code, if you have not received it please contact us at info@johnsnowlabs.com.\n",
    "\n",
    "<i>*If the next cell fails means your licensed enterprise version is not propertly installed so please check the pre-requisites guideline at http://www.johnsnowlabs.com/</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp_jsl.annotator import *\n",
    "import sparknlp_jsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup credentials to private JohnSnowLabs models repository with AWS-CLI\n",
    "\n",
    "Now is time to configure Spark-NLP in order to access private JohnSnowLabs models repository. This access is done via Amazon aws command line interface (AWSCLI).\n",
    "\n",
    "Instructions about how to install awscli are available at:\n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n",
    "\n",
    "Make sure you configure your credentials with aws configure following the instructions at:\n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n",
    "\n",
    "Please substitute the ACCESS_KEY and SECRET_KEY with the credentials you have recived. If you need your credentials contact us at info@johnsnowlabs.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup your license number in order to have access to private JohnSnowLabs models repository\n",
    "\n",
    "You need to setup the SPARK_NLP_LICENSE environment variable to the license number provided to you. If you need your license credentials contact us at info@johnsnowlabs.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NLP_LICENSE']='######'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Spark session\n",
    "\n",
    "The following will initialize the spark session in case you have run the jupyter notebook directly. If you have started the notebook using pyspark this cell is just ignored.\n",
    "\n",
    "Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded.\n",
    "\n",
    "The ####### is a secret code required to run the licensed version 2.4.0, if you have not received it please contact us at info@johnsnowlabs.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp_jsl.start(\"######\") # Secret code provided as part of the license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Context SpellChecker pipeline generation\n",
    "\n",
    "In Spark-NLP annotating NLP happens through pipelines. Pipelines are made out of various Annotator steps. In our case the architecture of the Context SpellChecker pipeline will be:\n",
    "\n",
    "* DocumentAssembler (text -> document)\n",
    "* SentenceDetector (document -> sentence)\n",
    "* Tokenizer (sentence -> token)\n",
    "* ContextSpellCheckerModel (token -> fixed)\n",
    "\n",
    "From the original text we generate a document and identify the different sentences. For each sentence the pipeline will extract the list of tokens and feed those to the context spellchecker.\n",
    "\n",
    "Finally we will use a pretrained model (ContextSpellCheckerModel) that is trained to fix mispelling errors based on contextual information.\n",
    "\n",
    "So from a text we will end having a list of tokens spellchecked in the \"fixed\" column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Initialize all the components of the pipeline\n",
    "The first three components are pretty straightforward Transformers/Annotators: DocumentAssembler, SentenceDetector and Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
    "\n",
    "document = DocumentAssembler()\\\n",
    ".setInputCol(\"text\")\\\n",
    ".setOutputCol(\"document\")\n",
    "\n",
    "# Rule based Sentence Detector annotator, processes various sentences per line\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "\n",
    "token = Tokenizer()\\\n",
    ".setInputCols([\"sentence\"])\\\n",
    ".setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth annotator in the pipeline will be the \"ContextSpellCheckerModel\". We will download a pretrained model only available in the Enterprise version named \"spellcheck_dl\".\n",
    "\n",
    "When running this cell your are advised to be patient. \n",
    "\n",
    "First time you call this pretrained model it needs to be downloaded in your local and it takes a while (depending on your internet connection).\n",
    "\n",
    "The size is about 69Mb and will be saved typically in your home folder as\n",
    "\n",
    "    ~HOMEFOLDER/cached_models/spellcheck_dl_en_2.0.2_2.4_1556479898829\n",
    "\n",
    "Next times you call it the model is loaded from your cached copy but even in that case it needs to be indexed each time so expect waiting up to 1 minutes (depending on your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_dl download started this may take some time.\n",
      "Approximate size to download 68.8 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "spellchecker_dl = ContextSpellCheckerModel.pretrained('spellcheck_dl', 'en', 'clinical/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also setup the name of the input column (\"token\", that is the output of the previous Annotator) and output column (\"fixed\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spellchecker_dl = spellchecker_dl.setInputCols([\"token\"])\\\n",
    ".setOutputCol(\"fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Defining the stages of the pipeline\n",
    "Now that we have created all the components of our pipeline, lets put all them together into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([\n",
    "    document,\n",
    "    sentenceDetector,\n",
    "    token,\n",
    "    spellchecker_dl\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Get your fitted model\n",
    "Now is time to fit our new pipeline. First we will create a Spark DataFrame including the sentence we want our SpellChecker to fix:\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "    I <span style=\"color: red\">habe</span> four in my family: Dad, Mum and <span style=\"color: red\">sisster</span>.<br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|text                                          |\n",
      "+----------------------------------------------+\n",
      "|I habe four in my family: Dad Mum and sisster.|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame([\n",
    "    [\"I habe four in my family: Dad Mum and sisster.\"]\n",
    "]).toDF('text')\n",
    "\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a model by fitting our pipeline to our content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Transform your data with the model to fix spelling errors.\n",
    "We will now apply the model transforming our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we will have a Spark DataFrame with a column containing the original tokens (\"token\") and another column with the fixed tokens (\"fixed\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|               fixed|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|I habe four in my...|[[document, 0, 45...|[[document, 0, 45...|[[token, 0, 0, I,...|[[token, 0, 0, I,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare both and see how our ContextSpellChecker has fixed the mispells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I habe four in my family : Dad Mum and sisster .\n",
      "I have four in my family : Dad Mum and sister .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(output.select('token.result').take(1)[0]['result']))\n",
    "print(\" \".join(output.select('fixed.result').take(1)[0]['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
