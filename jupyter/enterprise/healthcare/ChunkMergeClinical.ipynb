{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/enterprise/healthcare/ChunkMergeClinical.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MdE588BiY3z1",
    "outputId": "85d808ef-8b07-4819-8aab-05618f272d65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['secret', 'SPARK_NLP_LICENSE', 'JSL_OCR_LICENSE', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'JSL_OCR_SECRET'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('keys.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "license_keys.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "FVFdvGChZDDP",
    "outputId": "ecd2fc38-9b22-4079-8290-9728738d1689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.johnsnowlabs.com/l5rISdi5Xk\n",
      "Collecting spark-nlp-jsl==2.5.0\n",
      "  Downloading https://pypi.johnsnowlabs.com/l5rISdi5Xk/spark-nlp-jsl/spark_nlp_jsl-2.5.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pyspark==2.4.4 in /usr/local/lib/python3.6/dist-packages (from spark-nlp-jsl==2.5.0) (2.4.4)\n",
      "Collecting spark-nlp==2.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/b0/f50d169c49f5982f8be9e86e285b53e23f91fd7db0d10646c2d1de5c3ad0/spark_nlp-2.5.0-py2.py3-none-any.whl (120kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 7.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark==2.4.4->spark-nlp-jsl==2.5.0) (0.10.7)\n",
      "Installing collected packages: spark-nlp, spark-nlp-jsl\n",
      "Successfully installed spark-nlp-2.5.0 spark-nlp-jsl-2.5.0\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "\n",
    "secret = license_keys['secret']\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['JSL_OCR_LICENSE'] = license_keys['JSL_OCR_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID']= license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "! python -m pip install --upgrade spark-nlp-jsl==2.5.2  --extra-index-url https://pypi.johnsnowlabs.com/$secret\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed -q spark-nlp==2.5\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "print (sparknlp.version())\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "\n",
    "\n",
    "\n",
    "def start(secret):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"16G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0\") \\\n",
    "        .config(\"spark.jars\", \"https://pypi.johnsnowlabs.com/\"+secret+\"/spark-nlp-jsl-2.5.0.jar\")\n",
    "      \n",
    "    return builder.getOrCreate()\n",
    "\n",
    "\n",
    "spark = start(secret) # if you want to start the session with custom params as in start function above\n",
    "# sparknlp_jsl.start(secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zgsiTxjaiMd"
   },
   "outputs": [],
   "source": [
    "data_chunk_merge = spark.createDataFrame([\n",
    "  (1,\"\"\"A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to \n",
    "presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis \n",
    "three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index \n",
    "( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting.\n",
    "Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . \n",
    "She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . \n",
    "She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was \n",
    "significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , \n",
    "or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , \n",
    "anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin \n",
    "( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed \n",
    "as blood samples kept hemolyzing due to significant lipemia .\n",
    "The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior \n",
    "to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , \n",
    "the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , \n",
    "and lipase was 52 U/L .\n",
    " β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged \n",
    " and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . \n",
    " The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides \n",
    " to 1400 mg/dL , within 24 hours .\n",
    " Twenty days ago.\n",
    " Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . \n",
    " At birth the typical boy is growing slightly faster than the typical girl, but the velocities become equal at about \n",
    " seven months, and then the girl grows faster until four years. \n",
    " From then until adolescence no differences in velocity \n",
    " can be detected. 21-02-2020 \n",
    "21/04/2020\n",
    "\"\"\")]).toDF(\"id\",\"text\")\n",
    "\n",
    "\n",
    "regex = '''(c|p|yc|yp|r|rp|a)?(C[1-5])?M(x|X|0|1[a-d]?),pM\n",
    "(c|p|yc|yp|r|rp|a)?(C[1-5])?N(x|X|0|[1-3][a-d]?),pN\n",
    "(c|p|yc|yp|r|rp|a)?(C[1-5])?T(x|X|is|0|[1-4][a-d]?),pT\n",
    "([0-9]+(\\.[0-9]+)?\\s?x\\s?)*([0-9]+(\\.[0-9]+)?)\\s?(mg|MG|mm|cm|MM|CM|),SIZE\n",
    "at Los Angeles California,LOCATION\n",
    "Zacarias,PERSON\n",
    "better than,BLOCK'''\n",
    "\n",
    "with open('ner_regex.csv', 'w') as f:\n",
    "    f.write(regex)\n",
    "\n",
    "replace_dict = '''pT,TNM\n",
    "pM,TNM'''\n",
    "\n",
    "with open('replace_dict.csv', 'w') as f:\n",
    "    f.write(replace_dict)\n",
    "\n",
    "false_positives = '''beautiful thing,BLOCK'''\n",
    "\n",
    "with open('false_positives.csv', 'w') as f:\n",
    "    f.write(false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = '''male,man,male,boy,gentleman,he,him\n",
    "female,woman,female,girl,lady,old-lady,she,her\n",
    "neutral,neutral'''\n",
    "\n",
    "with open('gender.csv', 'w') as f:\n",
    "    f.write(gender)\n",
    "\n",
    "\n",
    "gender = {\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\", \n",
    "  \"completeMatchRegex\": \"true\"\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open('gender.json', 'w') as f:\n",
    "    json.dump(gender, f)\n",
    "\n",
    "\n",
    "date = {\n",
    "  \"entity\": \"Date \",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d{1,2}[\\\\/\\\\-\\\\:]{1}(\\\\d{1,2}[\\\\/\\\\-\\\\:]{1}){0,1}\\\\d{2,4}\",\n",
    "  \"valuesDefinition\":[],\n",
    "  \"prefix\": [],\n",
    "  \"suffix\": [],\n",
    "  \"contextLength\": 150,\n",
    "  \"context\": []\n",
    "}\n",
    "\n",
    "with open('date.json', 'w') as f:\n",
    "    json.dump(date, f)\n",
    "\n",
    "\n",
    "age = {\n",
    "  \"entity\": \"Age\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "   \"matchScope\":\"token\",\n",
    "  \"regex\":\"\\\\s*(0?[1-9]|[1-9][0-9]|[1][1-9][1-9]|200){1,2}[\\\\s-,]+|(?i)\\\\b(?:zero|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty)\\\\b(?=\\\\s*year)|\\\\b(?:(?:one|two|three|four|five|six|seven|eight|nine)? hundred(?:\\\\sand)?\\\\s)?(?:(?:twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety)[\\\\s-]?)?\\\\b(?:one|two|three|four|five|six|seven|eight|nine)?(?=\\\\syear)\",\n",
    "  \"prefix\":[\"age of\"],\n",
    "  \"suffix\": [\"-years-old\",\n",
    "             \"years-old\",\n",
    "             \"-year-old\",\n",
    "             \"-months-old\",\n",
    "             \"-month-old\",\n",
    "             \"-months-old\",\n",
    "             \"-day-old\",\n",
    "             \"-days-old\",\n",
    "             \"month old\",\n",
    "             \"days old\",\n",
    "             \"year old\",\n",
    "             \"years old\", \n",
    "             \"years\",\n",
    "             \"year\", \n",
    "             \"months\", \n",
    "             \"old\"\n",
    "              ],\n",
    "  \"contextLength\": 25,\n",
    "  \"context\": [],\n",
    "  \"contextException\": [\"ago\"],\n",
    "  \"exceptionDistance\": 10\n",
    "}\n",
    "\n",
    "with open('age.json', 'w') as f:\n",
    "    json.dump(age, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "weY5V9h7ZDf0",
    "outputId": "972b4569-744e-48dd-9471-920eb4d3e5af"
   },
   "outputs": [],
   "source": [
    "da = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "sd = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
    "tk = Tokenizer().setInputCols(\"sentence\").setOutputCol(\"token\")\n",
    "\n",
    "gender_contextual_parser = ContextualParserApproach() \\\n",
    "        .setInputCols([\"sentence\", \"token\"]) \\\n",
    "        .setOutputCol(\"entity_gender\") \\\n",
    "        .setJsonPath(\"gender.json\") \\\n",
    "        .setCaseSensitive(False) \\\n",
    "        .setContextMatch(False)\\\n",
    "        .setDictionary('gender.csv', read_as=ReadAs.TEXT, options={\"delimiter\":\",\"})\n",
    "        \n",
    "age_contextual_parser = ContextualParserApproach() \\\n",
    "        .setInputCols([\"sentence\", \"token\"]) \\\n",
    "        .setOutputCol(\"entity_age\") \\\n",
    "        .setJsonPath(\"age.json\") \\\n",
    "        .setCaseSensitive(False) \\\n",
    "        .setContextMatch(False)\n",
    "        \n",
    "date_contextual_parser = ContextualParserApproach() \\\n",
    "        .setInputCols([\"sentence\", \"token\"]) \\\n",
    "        .setOutputCol(\"entity_date\") \\\n",
    "        .setJsonPath(\"date.json\") \\\n",
    "        .setCaseSensitive(False) \\\n",
    "        .setContextMatch(False)\n",
    "\n",
    "merger1 = ChunkMergeApproach().setInputCols(\"entity_gender\",\"entity_age\").setOutputCol(\"combined\")\n",
    "merger2 = ChunkMergeApproach().setInputCols(\"combined\",\"entity_date\").setOutputCol(\"combined\")\n",
    "\n",
    "pl = Pipeline().setStages([da,sd,tk,gender_contextual_parser,age_contextual_parser,date_contextual_parser,merger1,merger2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pl.fit(data_chunk_merge).transform(data_chunk_merge).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "ogbBOST6aZTt",
    "outputId": "d8aff6ec-5976-4340-b002-c96adbe72e1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+------+\n",
      "|id |begin|ner_chunk|entity|\n",
      "+---+-----+---------+------+\n",
      "|1  |14   |female   |Gender|\n",
      "|1  |471  |she      |Gender|\n",
      "|1  |562  |she      |Gender|\n",
      "|1  |668  |she      |Gender|\n",
      "|1  |835  |her      |Gender|\n",
      "|1  |1377 |she      |Gender|\n",
      "|1  |1517 |her      |Gender|\n",
      "|1  |2097 |her      |Gender|\n",
      "|1  |2141 |her      |Gender|\n",
      "|1  |2236 |boy      |Gender|\n",
      "|1  |2284 |girl     |Gender|\n",
      "|1  |2360 |girl     |Gender|\n",
      "+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data.selectExpr(\"id\",\"explode(arrays_zip(entity_gender.begin,entity_gender.result, entity_gender.metadata)) as a\")\\\n",
    ".selectExpr(\"id\",\"a['0'] as begin\",\"a['1'] as ner_chunk\",\"a['2'].field as entity\")\\\n",
    ".orderBy(\"id\",\"begin\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+\n",
      "|id |begin|ner_chunk  |entity|\n",
      "+---+-----+-----------+------+\n",
      "|1  |2    |28-year-old|Age   |\n",
      "+---+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data.selectExpr(\"id\",\"explode(arrays_zip(entity_age.begin,entity_age.result, entity_age.metadata)) as a\")\\\n",
    ".selectExpr(\"id\",\"a['0'] as begin\",\"a['1'] as ner_chunk\",\"a['2'].field as entity\")\\\n",
    ".orderBy(\"id\",\"begin\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------+\n",
      "|id |begin|ner_chunk |entity|\n",
      "+---+-----+----------+------+\n",
      "|1  |2472 |21-02-2020|Date  |\n",
      "|1  |2484 |21/04/2020|Date  |\n",
      "+---+-----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data.selectExpr(\"id\",\"explode(arrays_zip(entity_date.begin,entity_date.result, entity_date.metadata)) as a\")\\\n",
    ".selectExpr(\"id\",\"a['0'] as begin\",\"a['1'] as ner_chunk\",\"a['2'].field as entity\")\\\n",
    ".orderBy(\"id\",\"begin\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpLba4tAbPiW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+\n",
      "|id |chunk      |entity|\n",
      "+---+-----------+------+\n",
      "|1  |28-year-old|Age   |\n",
      "|1  |female     |Gender|\n",
      "|1  |she        |Gender|\n",
      "|1  |she        |Gender|\n",
      "|1  |she        |Gender|\n",
      "|1  |her        |Gender|\n",
      "|1  |she        |Gender|\n",
      "|1  |her        |Gender|\n",
      "|1  |her        |Gender|\n",
      "|1  |her        |Gender|\n",
      "|1  |boy        |Gender|\n",
      "|1  |girl       |Gender|\n",
      "|1  |girl       |Gender|\n",
      "|1  |21-02-2020 |Date  |\n",
      "|1  |21/04/2020 |Date  |\n",
      "+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data.selectExpr(\"id\",\"explode(arrays_zip(combined.result, combined.metadata)) as a\")\\\n",
    ".selectExpr(\"id\",\"a['0'] as chunk\",\"a['1'].entity as entity\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ChunkMergeClinical.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jsl250",
   "language": "python",
   "name": "jsl250"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
