{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5O9HSDTQzeFR"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/enterprise/healthcare/DeIdentification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICHD-Gary_6m"
   },
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i8U-tz4lzttG",
    "outputId": "08446d44-a028-4648-cbca-8ff7099a5ce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['secret', 'SPARK_NLP_LICENSE', 'JSL_OCR_LICENSE', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'JSL_OCR_SECRET'])"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('keys.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "license_keys.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "jD5kLq1yzvCv",
    "outputId": "a6897660-cb88-428b-8abc-0cbdf0dfcc4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.johnsnowlabs.com/9hk9l8ybo1\n",
      "Collecting spark-nlp-jsl==2.5.1rc1\n",
      "  Downloading https://pypi.johnsnowlabs.com/9hk9l8ybo1/spark-nlp-jsl/spark_nlp_jsl-2.5.1rc1-py3-none-any.whl\n",
      "Collecting pyspark==2.4.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 57kB/s \n",
      "\u001b[?25hCollecting spark-nlp==2.5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/b4/db653f8080a446de8ce981b262d85c85c61de7e920930726da0d1c6b4c65/spark_nlp-2.5.1-py2.py3-none-any.whl (121kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 45.8MB/s \n",
      "\u001b[?25hCollecting py4j==0.10.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 34.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130388 sha256=26cf12d0008b527533e10c6e3cce247848676c77ee7e8f154b7ffe67ab77fd47\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark, spark-nlp, spark-nlp-jsl\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4 spark-nlp-2.5.1 spark-nlp-jsl-2.5.1rc1\n",
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "secret = license_keys['secret']\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['JSL_OCR_LICENSE'] = license_keys['JSL_OCR_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID']= license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "version = license_keys['version']\n",
    "\n",
    "! python -m pip install --upgrade spark-nlp-jsl==$version  --extra-index-url https://pypi.johnsnowlabs.com/$secret\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "print (sparknlp.version())\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "\n",
    "\n",
    "\n",
    "def start(secret):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"16G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:\"+version) \\\n",
    "        .config(\"spark.jars\", \"https://pypi.johnsnowlabs.com/\"+secret+\"/spark-nlp-jsl-\"+version+\".jar\")\n",
    "      \n",
    "    return builder.getOrCreate()\n",
    "\n",
    "\n",
    "spark = start(secret) # if you want to start the session with custom params as in start function above\n",
    "# sparknlp_jsl.start(secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDflYvM4y_6p"
   },
   "source": [
    "# DeIdentification - version 2.5.1\n",
    "\n",
    "## Example for Named Entity Recognition with De-Identification Pipeline\n",
    "\n",
    "One of the major issues when it comes to the analysis of medical records is how to deal with the confidentiality nature of the content.\n",
    "\n",
    "Lets imagine we have a clinical record that contains this heading:\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "Record date: 2093-01-13<br>\n",
    "David Hale, M.D.<br>\n",
    "Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira<br>\n",
    "Record date: 2079-11-09. Cocke County Baptist Hospital. 0295 Keats Street.<br>\n",
    "</div>\n",
    "\n",
    "A usual requisite is to remove or ofuscate any content fragment that contains or potentially containts data that could be linked to an individual as for instance:\n",
    "- Names and surnames of the patient\n",
    "- Names and surnames of the doctors\n",
    "- Name of a medical center\n",
    "- Name of a City or Town\n",
    "- Street adress\n",
    "- Telephone number\n",
    "- e-mail\n",
    "- Date of birth (because combined with other data could lead to identification of patients)\n",
    "- etc...\n",
    "\n",
    "SparkNLP Enterprise provides with pipeline functionalities that allow to locate those fragments with personal sensible information and anonimize if required. We will see in this notebook an example of such a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOAyNt4Xy_7w"
   },
   "source": [
    "### Step 1: De-identification pipeline generation\n",
    "\n",
    "In Spark-NLP annotating NLP happens through pipelines. Pipelines are made out of various Annotator steps. In our case the architecture of the De-identification pipeline will be:\n",
    "\n",
    "* DocumentAssembler (text -> document)\n",
    "* SentenceDetector (document -> sentence)\n",
    "* Tokenizer (sentence -> token)\n",
    "* WordEmbeddingsModel ([sentence, token] -> embeddings)\n",
    "* NerDLModel (deidentify_dl) ([sentence, token, embeddings] -> ner)\n",
    "* NerConverter ([sentence, token, ner] -> ner_chunk)\n",
    "* DeIdentificationModel ([sentence, token, ner_chunk] -> deidentified\n",
    "\n",
    "So from a text we end having a deidentified text.\n",
    "\n",
    "We will use a pretrained model (NerDLModel deidentify) that uses wordembeddings to recognize tokens that contains personal information. Then we transform its output (ner) into ner_chunk that is then used by another pretrained annotator (DeIdentificationModel) that will finally generate a deidentified text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4bBkMliy_7x"
   },
   "source": [
    "#### Step 1.1 Load all the components of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mEYwmzvWy_7z"
   },
   "outputs": [],
   "source": [
    "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "# Sentence Detector annotator, processes various sentences per line\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols([\"sentence\"])\\\n",
    "  .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GsReIPry_7_"
   },
   "source": [
    "The fourth annotator in the pipeline is \"WordEmbeddingsModel\". We will download a pretrained model available from \"clinical/models\" named \"embeddings_clinical\".\n",
    "\n",
    "When running this cell your are advised to be patient. \n",
    "\n",
    "First time you call this pretrained model it needs to be downloaded in your local and it takes a while.\n",
    "\n",
    "The size is about 1.7Gb and will be saved typically in your home folder as\n",
    "\n",
    "    ~HOMEFOLDER/cached_models/embeddings_clinical_en_2.0.2_2.4_1558454742956.zip\n",
    "\n",
    "Next times you call it the model is loaded from your cached copy but even in that case it needs to be indexed each time so expect waiting up to 5 minutes (depending on your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jccmwbwBy_8A",
    "outputId": "f18d8bcd-03a4-43c8-a797-6460152a1f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "  .setInputCols([\"sentence\", \"token\"])\\\n",
    "  .setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "uxJYkX_Ry_8M",
    "outputId": "9b064700-db38-4fe3-8977-4c45a58f0b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_deid_large download started this may take some time.\n",
      "Approximate size to download 14 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition for clinical sensitive information. Includes names, phone numbers, addresses, etc\n",
    "\n",
    "clinical_sensitive_entities = NerDLModel.pretrained(\"ner_deid_large\", \"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "  .setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZzdcujJy_8T"
   },
   "outputs": [],
   "source": [
    "# Named Entity Recognition concepts parser, transforms entities into CHUNKS (required for next step: assertion status)\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "  .setOutputCol(\"ner_chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wDF_6mzXy_8b",
    "outputId": "e57c149a-1bf4-4344-884c-c3df268033f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deidentify_large download started this may take some time.\n",
      "Approximate size to download 54.7 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "deidentification_rules = DeIdentificationModel.pretrained(\"deidentify_large\", \"en\", \"clinical/models\") \\\n",
    "  .setMode(\"obfuscate\") \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"ner_chunk\"]) \\\n",
    "  .setOutputCol(\"deidentified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtPJyUfYy_8h"
   },
   "source": [
    "#### Step 1.2 Defining the stages of the pipeline\n",
    "Now that we have created all the components of our pipeline, lets put all them together into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZvvYmFDy_8i"
   },
   "outputs": [],
   "source": [
    "# Build up the pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    clinical_sensitive_entities,\n",
    "    ner_converter,\n",
    "    deidentification_rules\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ainShoDIy_8o"
   },
   "source": [
    "### Step 2 Get your model by fitting the pipeline with some data\n",
    "Lest now see how our Deidientification pipeline works with some data. We will use the following data containing personal information as an example:\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "Record date: 2093-01-13<br>\n",
    "David Hale, M.D.<br>\n",
    "Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira<br>\n",
    "Record date: 2079-11-09. Cocke County Baptist Hospital. 0295 Keats Street.<br>\n",
    "</div>\n",
    "\n",
    "We will create a Spark DataFrame containing the lines of this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "P17e0UXGy_8p",
    "outputId": "8d725252-7400-48a0-ad3b-8b529e465169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+\n",
      "|text                                                                      |\n",
      "+--------------------------------------------------------------------------+\n",
      "|Record date: 2093-01-13                                                   |\n",
      "|David Hale, M.D.                                                          |\n",
      "|Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira          |\n",
      "|Record date: 2079-11-09. Cocke County Baptist Hospital. 0295 Keats Street.|\n",
      "+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We want to know more about this simple dataframe\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "  [\"Record date: 2093-01-13\"],\n",
    "  [\"David Hale, M.D.\"],\n",
    "  [\"Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira\"],\n",
    "  [\"Record date: 2079-11-09. Cocke County Baptist Hospital. 0295 Keats Street.\"]\n",
    "]).toDF(\"text\")\n",
    "\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5FglqRwEy_8y"
   },
   "source": [
    "Now we will create a model by fitting our pipeline to our content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPpWbYi_y_8z"
   },
   "outputs": [],
   "source": [
    "# We convert the pipeline into a model, train any annotator if required (not the case)\n",
    "\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USgReNNjy_8-"
   },
   "source": [
    "### Step 3. Transform your data with the model to deidentify content.\n",
    "As a next step we transform our content using the new model generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7RMO92Gy_9A"
   },
   "outputs": [],
   "source": [
    "output = model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYAeLkQ9y_9G"
   },
   "source": [
    "Lets compare the original sentence ('sentence.result') with the final deidentified text ('deidentified.result') generated by the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "JZKMdbxpy_9J",
    "outputId": "6031941c-e2e9-45bb-8f10-b5a7f58eb402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences:\n",
      "+------------------------------------------------------------------------------+\n",
      "|result                                                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "|[Record date: 2093-01-13]                                                     |\n",
      "|[David Hale, M.D.]                                                            |\n",
      "|[Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira]            |\n",
      "|[Record date: 2079-11-09., Cocke County Baptist Hospital., 0295 Keats Street.]|\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "Annonymized output:\n",
      "+---------------------------------------------------+\n",
      "|result                                             |\n",
      "+---------------------------------------------------+\n",
      "|[Record date: 2093-01-18]                          |\n",
      "|[IRA, M.D.]                                        |\n",
      "|[Name: JENS MR. #7194334 Date: <DATE> PCP: HORATIA]|\n",
      "|[Record date: 2079-12-12., HAWTHORNE., FRANKTOWN.] |\n",
      "+---------------------------------------------------+\n",
      "\n",
      "CPU times: user 7.34 ms, sys: 77 µs, total: 7.42 ms\n",
      "Wall time: 898 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply the actual transformation\n",
    "\n",
    "print(\"Original sentences:\")\n",
    "output.select(\"sentence.result\").show(truncate=False)\n",
    "print(\"Annonymized output:\")\n",
    "output.select(\"deidentified.result\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NR93XATy_9Q"
   },
   "source": [
    "Surnames, dates, names of healthcare facilities and street address have been identified as a potential personal information and substitued by generic masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEJtDukvy_9R"
   },
   "source": [
    "### Step 4 with LightPipelines\n",
    "\n",
    "Once you have created a model by fitting a pipeline with some data you can leverage the use of LightPipelines, faster and easier to use for testing or real-time queries.\n",
    "\n",
    "Lets created a light_pipeline from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdAi3Rs2y_9S"
   },
   "outputs": [],
   "source": [
    "light_pipeline = LightPipeline(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APA-HGeyy_9Y"
   },
   "source": [
    "Now by just calling the method .annotate of our light_pipeline we will deidentify any content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HswnRSVly_9Z",
    "outputId": "319cbf7a-2325-4e5f-efa9-98e3e3bf0f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Smith García, DOB: 23/07/1977 Dr. Suarez. 17 Main Street, Miami Hospital, USA\n",
      "Name: <NAME>, DOB: <DATE> Dr. <NAME>.<LOCATION>, <LOCATION>, <LOCATION>\n"
     ]
    }
   ],
   "source": [
    "# Call annotate() in order to test a sentence or a list of sentences\n",
    "ori_str = \"Name: Smith García, DOB: 23/07/1977 Dr. Suarez. 17 Main Street, Miami Hospital, USA\"\n",
    "light_data = light_pipeline.annotate(ori_str)\n",
    "print(ori_str)\n",
    "print(\"\".join(light_data['deidentified']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gvt2lWSSy_9f"
   },
   "source": [
    "Here we can how the NERDl for deidentification assigns the different NER classes to the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "CvEeiDETy_9g",
    "outputId": "64f45d8c-c0ad-4308-e228-d7bece82083f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN (NER)\n",
      "============\n",
      "Name (O)\n",
      "------------\n",
      ": (O)\n",
      "------------\n",
      "Smith (B-NAME)\n",
      "------------\n",
      "García (I-NAME)\n",
      "------------\n",
      ", (O)\n",
      "------------\n",
      "DOB (O)\n",
      "------------\n",
      ": (O)\n",
      "------------\n",
      "23/07/1977 (B-DATE)\n",
      "------------\n",
      "Dr (O)\n",
      "------------\n",
      ". (O)\n",
      "------------\n",
      "Suarez (B-NAME)\n",
      "------------\n",
      ". (O)\n",
      "------------\n",
      "17 (B-LOCATION)\n",
      "------------\n",
      "Main (I-LOCATION)\n",
      "------------\n",
      "Street (I-LOCATION)\n",
      "------------\n",
      ", (O)\n",
      "------------\n",
      "Miami (B-LOCATION)\n",
      "------------\n",
      "Hospital (I-LOCATION)\n",
      "------------\n",
      ", (O)\n",
      "------------\n",
      "USA (B-LOCATION)\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "print(\"TOKEN (NER)\")\n",
    "print(\"============\")\n",
    "for i in range(len(light_data['token'])):\n",
    "    print(light_data['token'][i] + \" (\" + light_data['ner'][i]+\")\")\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ES4HS_d1y_9m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DeIdentification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
