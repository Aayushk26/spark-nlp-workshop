{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeIdentification - version 2.4.0\n",
    "\n",
    "## Example for Named Entity Recognition with De-Identification Pipeline\n",
    "\n",
    "One of the major issues when it comes to the analysis of medical records is how to deal with the confidentiality nature of the content.\n",
    "\n",
    "Lets imagine we have a clinical record that contains this heading:\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "Record date: 2093-01-13<br>\n",
    "David Hale, M.D.<br>\n",
    "Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira<br>\n",
    "Record date: 2079-11-09. Cocke County Baptist Hospital. 0295 Keats Street.<br>\n",
    "</div>\n",
    "\n",
    "A usual requisite is to remove or ofuscate any content fragment that contains or potentially containts data that could be linked to an individual as for instance:\n",
    "- Names and surnames of the patient\n",
    "- Names and surnames of the doctors\n",
    "- Name of a medical center\n",
    "- Name of a City or Town\n",
    "- Street adress\n",
    "- Telephone number\n",
    "- e-mail\n",
    "- Date of birth (because combined with other data could lead to identification of patients)\n",
    "- etc...\n",
    "\n",
    "SparkNLP Enterprise provides with pipeline functionalities that allow to locate those fragments with personal sensible information and anonimize if required. We will see in this notebook an example of such a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Prepare the environment\n",
    "\n",
    "#### Install OpenSource spark-nlp and pyspark pip packages\n",
    "As a first step we import the required python dependences including some sparknlp components.\n",
    "\n",
    "Be sure that you have the required python libraries (pyspark 2.4.4, spark-nlp 2.4.0) by running <code>pip list</code>. Check that the versions are correct.\n",
    "\n",
    "If some of them is missing you can run:\n",
    "\n",
    "<code>pip install --ignore-installed pyspark==2.4.4</code><br>\n",
    "<code>pip install --ignore-installed spark-nlp==2.4.0</code><br>\n",
    "\n",
    "The --ignore-installed parameter is to overwrite your previous pip package version if already installed.\n",
    "\n",
    "<i>*If this cell fails means you have not propertly setup the required environment. Please check the pre-requisites guideline at http://www.johnsnowlabs.com</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e9cd9585ee0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/python/venvs/jslws/lib/python3.6/site-packages/sparknlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocumentAssembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFinisher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenAssembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChunk2Doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoc2Chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import sys, time\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.util import *\n",
    "from sparknlp.embeddings import *\n",
    "\n",
    "from sparknlp.embeddings import EmbeddingsHelper\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Licensed Sparl-NLP package\n",
    "\n",
    "We will use also some Spark-NLP enterprise functionalities contained in the spark-nlp-jsl package.\n",
    "\n",
    "You can check that spark-nlp-jsl is installed by running <code>pip install</code>. Check that version installed is 2.4.0\n",
    "\n",
    "If it is not then you need to install it by using:\n",
    "\n",
    "<code>pip install spark-nlp-jsl==2.4.0 --extra-index-url https://pypi.johnsnowlabs.com/##### --ignore-installed</code>\n",
    "\n",
    "The ##### is a secret url, if you have not received it please contact us at info@johnsnowlabs.com.\n",
    "\n",
    "<i>*If the next cell fails means your licensed enterprise version is not propertly installed so please check the pre-requisites guideline at http://www.johnsnowlabs.com/</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this fails, means pip module for enterprise has not been properly set up\n",
    "\n",
    "from sparknlp_jsl.annotator import *\n",
    "import sparknlp_jsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup credentials to private JohnSnowLabs models repository with AWS-CLI\n",
    "\n",
    "Now is time to configure Spark-NLP in order to access private JohnSnowLabs models repository. This access is done via Amazon aws command line interface (AWSCLI).\n",
    "\n",
    "Instructions about how to install awscli are available at: \n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n",
    "\n",
    "Make sure you configure your credentials with <code>aws configure</code> following the instructions at:\n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n",
    "\n",
    "Please substitute the ACCESS_KEY and SECRET_KEY with the credentials you have recived. If you need your credentials contact us at info@johnsnowlabs.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup your license number in order to have access to private JohnSnowLabs models repository\n",
    "\n",
    "You need to setup the SPARK_NLP_LICENSE environment variable to the license number provided to you. If you need your license credentials contact us at info@johnsnowlabs.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NLP_LICENSE']='######'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Spark session\n",
    "\n",
    "The following will initialize the spark session in case you have run the jupyter notebook directly. If you have started the notebook using pyspark this cell is just ignored.\n",
    "\n",
    "Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded.\n",
    "\n",
    "We will be using version 2.4.0 of Spark NLP Open Source and 2.4.0 of Spark NLP Enterprise Edition.\n",
    "\n",
    "The #### in <code>.config(\"spark.jars\", \"####\")</code> is a secret url, if you have not received it please contact us at info@johnsnowlabs.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp_jsl.start(\"#####\") # Secret code provided as part of the license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: De-identification pipeline generation\n",
    "\n",
    "In Spark-NLP annotating NLP happens through pipelines. Pipelines are made out of various Annotator steps. In our case the architecture of the De-identification pipeline will be:\n",
    "\n",
    "* DocumentAssembler (text -> document)\n",
    "* SentenceDetector (document -> sentence)\n",
    "* Tokenizer (sentence -> token)\n",
    "* WordEmbeddingsModel ([sentence, token] -> embeddings)\n",
    "* NerDLModel (deidentify_dl) ([sentence, token, embeddings] -> ner)\n",
    "* NerConverter ([sentence, token, ner] -> ner_chunk)\n",
    "* DeIdentificationModel ([sentence, token, ner_chunk] -> deidentified\n",
    "\n",
    "So from a text we end having a deidentified text.\n",
    "\n",
    "We will use a pretrained model (NerDLModel deidentify) that uses wordembeddings to recognize tokens that contains personal information. Then we transform its output (ner) into ner_chunk that is then used by another pretrained annotator (DeIdentificationModel) that will finally generate a deidentified text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Load all the components of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "# Sentence Detector annotator, processes various sentences per line\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols([\"sentence\"])\\\n",
    "  .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth annotator in the pipeline is \"WordEmbeddingsModel\". We will download a pretrained model available from \"clinical/models\" named \"embeddings_clinical\".\n",
    "\n",
    "When running this cell your are advised to be patient. \n",
    "\n",
    "First time you call this pretrained model it needs to be downloaded in your local and it takes a while.\n",
    "\n",
    "The size is about 1.7Gb and will be saved typically in your home folder as\n",
    "\n",
    "    ~HOMEFOLDER/cached_models/embeddings_clinical_en_2.0.2_2.4_1558454742956.zip\n",
    "\n",
    "Next times you call it the model is loaded from your cached copy but even in that case it needs to be indexed each time so expect waiting up to 5 minutes (depending on your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "  .setInputCols([\"sentence\", \"token\"])\\\n",
    "  .setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the \"deidentify\" NerDLModel is about 15Mb and will be saved typically in your home folder as\n",
    "\n",
    "    ~HOMEFOLDER/cached_models/deidentify_dl_en_2.0.2_2.4_1559669094458.zip\n",
    "\n",
    "Next times you call it the model is loaded from your cached copy and then usually takes few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deidentify_dl download started this may take some time.\n",
      "Approximate size to download 14.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition for clinical sensitive information. Includes names, phone numbers, addresses, etc\n",
    "\n",
    "clinical_sensitive_entities = NerDLModel.pretrained(\"deidentify_dl\", \"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "  .setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition concepts parser, transforms entities into CHUNKS (required for next step: assertion status)\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "  .setOutputCol(\"ner_chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the \"deidentify\" NerDLModel is about 4Kb and will be saved typically in your home folder as\n",
    "\n",
    "    ~HOMEFOLDER/cached_models/deidentify_rb_en_2.0.2_2.4_1559672122511.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deidentify_rb download started this may take some time.\n",
      "Approximate size to download 3.8 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "deidentification_rules = DeIdentificationModel.pretrained(\"deidentify_rb\", \"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"ner_chunk\"]) \\\n",
    "  .setOutputCol(\"deidentified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Defining the stages of the pipeline\n",
    "Now that we have created all the components of our pipeline, lets put all them together into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    clinical_sensitive_entities,\n",
    "    ner_converter,\n",
    "    deidentification_rules\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Get your model by fitting the pipeline with some data\n",
    "Lest now see how our Deidientification pipeline works with some data. We will use the following data containing personal information as an example:\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "Record date: 2093-01-13<br>\n",
    "David Hale, M.D.<br>\n",
    "Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira<br>\n",
    "Record date: 2079-11-09. Cocke County Baptist Hospital. 0295 Keats Street.<br>\n",
    "</div>\n",
    "\n",
    "We will create a Spark DataFrame containing the lines of this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+\n",
      "|text                                                                      |\n",
      "+--------------------------------------------------------------------------+\n",
      "|Record date: 2093-01-13                                                   |\n",
      "|David Hale, M.D.                                                          |\n",
      "|Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira          |\n",
      "|Record date: 2079-11-09. Cocke County Baptist Hospital. 0295 Keats Street.|\n",
      "+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We want to know more about this simple dataframe\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "  [\"Record date: 2093-01-13\"],\n",
    "  [\"David Hale, M.D.\"],\n",
    "  [\"Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira\"],\n",
    "  [\"Record date: 2079-11-09. Cocke County Baptist Hospital. 0295 Keats Street.\"]\n",
    "]).toDF(\"text\")\n",
    "\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a model by fitting our pipeline to our content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the pipeline into a model, train any annotator if required (not the case)\n",
    "\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Transform your data with the model to deidentify content.\n",
    "As a next step we transform our content using the new model generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the original sentence ('sentence.result') with the final deidentified text ('deidentified.result') generated by the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences:\n",
      "+------------------------------------------------------------------------------+\n",
      "|result                                                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "|[Record date: 2093-01-13]                                                     |\n",
      "|[David Hale, M.D.]                                                            |\n",
      "|[Name: Hendrickson, Ora MR. #7194334 Date: 01/13/93 PCP: Oliveira]            |\n",
      "|[Record date: 2079-11-09., Cocke County Baptist Hospital., 0295 Keats Street.]|\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "Annonymized output:\n",
      "+---------------------------------------------------------------------+\n",
      "|result                                                               |\n",
      "+---------------------------------------------------------------------+\n",
      "|[Record date: 2093-01-13]                                            |\n",
      "|[<DOCTOR>, M.D.]                                                     |\n",
      "|[Name: Hendrickson, <DOCTOR> MR. #7194334 Date: <DATE> PCP: <DOCTOR>]|\n",
      "|[Record date: 2079-11-09., <HOSPITAL>., <STREET>.]                   |\n",
      "+---------------------------------------------------------------------+\n",
      "\n",
      "CPU times: user 3.7 ms, sys: 4.37 ms, total: 8.07 ms\n",
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply the actual transformation\n",
    "\n",
    "print(\"Original sentences:\")\n",
    "output.select(\"sentence.result\").show(truncate=False)\n",
    "print(\"Annonymized output:\")\n",
    "output.select(\"deidentified.result\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surnames, dates, names of healthcare facilities and street address have been identified as a potential personal information and substitued by generic masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 with LightPipelines\n",
    "\n",
    "Once you have created a model by fitting a pipeline with some data you can leverage the use of LightPipelines, faster and easier to use for testing or real-time queries.\n",
    "\n",
    "Lets created a light_pipeline from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_pipeline = LightPipeline(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by just calling the method .annotate of our light_pipeline we will deidentify any content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Smith García, DOB: 23/07/1977 Dr. Suarez. 17 Main Street, Miami Hospital, USA\n",
      "Name: Smith <PATIENT>, DOB: 12/08/1977 Dr. <PATIENT>.17 <HOSPITAL>, <HOSPITAL>, <HOSPITAL>\n"
     ]
    }
   ],
   "source": [
    "# Call annotate() in order to test a sentence or a list of sentences\n",
    "ori_str = \"Name: Smith García, DOB: 23/07/1977 Dr. Suarez. 17 Main Street, Miami Hospital, USA\"\n",
    "light_data = light_pipeline.annotate(ori_str)\n",
    "print(ori_str)\n",
    "print(\"\".join(light_data['deidentified']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can how the NERDl for deidentification assigns the different NER classes to the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN (NER)\n",
      "============\n",
      "Name (O)\n",
      "------------\n",
      ": (O)\n",
      "------------\n",
      "Smith (O)\n",
      "------------\n",
      "García (I-PATIENT)\n",
      "------------\n",
      ", (O)\n",
      "------------\n",
      "DOB (O)\n",
      "------------\n",
      ": (O)\n",
      "------------\n",
      "23/07/1977 (I-DATE)\n",
      "------------\n",
      "Dr (O)\n",
      "------------\n",
      ". (O)\n",
      "------------\n",
      "Suarez (I-PATIENT)\n",
      "------------\n",
      ". (O)\n",
      "------------\n",
      "17 (O)\n",
      "------------\n",
      "Main (I-HOSPITAL)\n",
      "------------\n",
      "Street (I-HOSPITAL)\n",
      "------------\n",
      ", (O)\n",
      "------------\n",
      "Miami (I-HOSPITAL)\n",
      "------------\n",
      "Hospital (I-HOSPITAL)\n",
      "------------\n",
      ", (O)\n",
      "------------\n",
      "USA (I-HOSPITAL)\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "print(\"TOKEN (NER)\")\n",
    "print(\"============\")\n",
    "for i in range(len(light_data['token'])):\n",
    "    print(light_data['token'][i] + \" (\" + light_data['ner'][i]+\")\")\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
