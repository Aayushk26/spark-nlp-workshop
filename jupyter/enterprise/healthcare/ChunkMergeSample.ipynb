{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lc_L6EaAWEzo"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/enterprise/healthcare/ChunkMergeSample.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MdE588BiY3z1",
    "outputId": "d9bb9377-53f1-4615-d325-2312999c3c73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['secret', 'SPARK_NLP_LICENSE', 'JSL_OCR_LICENSE', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'JSL_OCR_SECRET'])"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('keys.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "license_keys.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "FVFdvGChZDDP",
    "outputId": "d54e5446-3bb4-4714-edfc-1e9c53c418ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 62kB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 30.1MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.johnsnowlabs.com/l5rISdi5Xk\n",
      "Collecting spark-nlp-jsl==2.5.0\n",
      "  Downloading https://pypi.johnsnowlabs.com/l5rISdi5Xk/spark-nlp-jsl/spark_nlp_jsl-2.5.0-py3-none-any.whl\n",
      "Collecting spark-nlp==2.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/b0/f50d169c49f5982f8be9e86e285b53e23f91fd7db0d10646c2d1de5c3ad0/spark_nlp-2.5.0-py2.py3-none-any.whl (120kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 6.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyspark==2.4.4 in /usr/local/lib/python3.6/dist-packages (from spark-nlp-jsl==2.5.0) (2.4.4)\n",
      "Requirement already satisfied, skipping upgrade: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark==2.4.4->spark-nlp-jsl==2.5.0) (0.10.7)\n",
      "Installing collected packages: spark-nlp, spark-nlp-jsl\n",
      "Successfully installed spark-nlp-2.5.0 spark-nlp-jsl-2.5.0\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "\n",
    "secret = license_keys['secret']\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['JSL_OCR_LICENSE'] = license_keys['JSL_OCR_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID']= license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "! python -m pip install --upgrade spark-nlp-jsl==2.5.0  --extra-index-url https://pypi.johnsnowlabs.com/$secret\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed -q spark-nlp==2.5\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "print (sparknlp.version())\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "\n",
    "\n",
    "\n",
    "def start(secret):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"16G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0\") \\\n",
    "        .config(\"spark.jars\", \"https://pypi.johnsnowlabs.com/\"+secret+\"/spark-nlp-jsl-2.5.0.jar\")\n",
    "      \n",
    "    return builder.getOrCreate()\n",
    "\n",
    "\n",
    "spark = start(secret) # if you want to start the session with custom params as in start function above\n",
    "# sparknlp_jsl.start(secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zgsiTxjaiMd"
   },
   "outputs": [],
   "source": [
    "data_chunk_merge = spark.createDataFrame([\n",
    "  (1,\"Zacarias Woods would not have T2N1 at Los Angeles California where he presented lymphocite leukimia\",),\n",
    "  (2,\"Andre Agassi had 2 x 3 x 1 mm hairwig better than T1N2M1 with adenocarcinoma\",)\n",
    "]).toDF(\"id\",\"text\")\n",
    "\n",
    "regex = '''(c|p|yc|yp|r|rp|a)?(C[1-5])?M(x|X|0|1[a-d]?),pM\n",
    "(c|p|yc|yp|r|rp|a)?(C[1-5])?N(x|X|0|[1-3][a-d]?),pN\n",
    "(c|p|yc|yp|r|rp|a)?(C[1-5])?T(x|X|is|0|[1-4][a-d]?),pT\n",
    "(c|p|yc|yp|r|rp|a)?(C[1-5])?T(x|X|is|0|[1-4][a-d]?),pT\n",
    "([0-9]+(\\.[0-9]+)?\\s?x\\s?)*([0-9]+(\\.[0-9]+)?)\\s?(mg|MG|mm|cm|MM|CM|),SIZE\n",
    "T1N2M1,TNM\n",
    "at Los Angeles California,LOCATION\n",
    "Zacarias,PERSON\n",
    "better than,BLOCK'''\n",
    "\n",
    "with open('ner_regex.csv', 'w') as f:\n",
    "    f.write(regex)\n",
    "\n",
    "replace_dict = '''pT,TNM\n",
    "pM,TNM'''\n",
    "\n",
    "with open('replace_dict.csv', 'w') as f:\n",
    "    f.write(replace_dict)\n",
    "\n",
    "false_positives = '''better than,BLOCK'''\n",
    "\n",
    "with open('false_positives.csv', 'w') as f:\n",
    "    f.write(false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "weY5V9h7ZDf0",
    "outputId": "7ea64362-5f81-4f1c-a636-70ec86067069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_deid_large download started this may take some time.\n",
      "Approximate size to download 14 MB\n",
      "[OK!]\n",
      "ner_bionlp download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "da = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "sd = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
    "tk = Tokenizer().setInputCols(\"sentence\").setOutputCol(\"token\")\n",
    "emb = WordEmbeddingsModel.pretrained(\"embeddings_clinical\",\"en\",\"clinical/models\").setOutputCol(\"embs\")\n",
    "ner = NerDLModel.pretrained(\"ner_deid_large\",\"en\",\"clinical/models\").setInputCols(\"sentence\",\"token\",\"embs\").setOutputCol(\"ner\")\n",
    "canner = NerDLModel.pretrained(\"ner_bionlp\",\"en\",\"clinical/models\").setInputCols(\"sentence\",\"token\",\"embs\").setOutputCol(\"canner\")\n",
    "nc = NerConverter().setInputCols(\"sentence\",\"token\",\"ner\").setOutputCol(\"ner_chunk\")\n",
    "cannc = NerConverter().setInputCols(\"sentence\",\"token\",\"canner\").setOutputCol(\"canner_chunk\")\n",
    "rex = RegexMatcher().setInputCols(\"sentence\").setOutputCol(\"rex\").setExternalRules(\"ner_regex.csv\",\",\",\"TEXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "merger_can = ChunkMergeApproach().setInputCols(\"ner_chunk\",\"canner_chunk\").setOutputCol(\"combined\")\\\n",
    "    .setFalsePositivesResource(\"false_positives.csv\",\"TEXT\", {\"delimiter\":\",\"})\\\n",
    "    .setReplaceDictResource(\"replace_dict.csv\",\"TEXT\", {\"delimiter\":\",\"})\n",
    "\n",
    "merger_rex = ChunkMergeApproach().setInputCols(\"combined\",\"rex\").setOutputCol(\"combined\")\\\n",
    "    .setFalsePositivesResource(\"false_positives.csv\",\"TEXT\", {\"delimiter\":\",\"})\\\n",
    "    .setReplaceDictResource(\"replace_dict.csv\",\"TEXT\", {\"delimiter\":\",\"})\\\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "pl = Pipeline().setStages([da,sd,tk,emb,ner,canner,nc,cannc,rex,merger_can, merger_rex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jVGpbBxWEz1"
   },
   "outputs": [],
   "source": [
    "merged_data = pl.fit(data_chunk_merge).transform(data_chunk_merge).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "ogbBOST6aZTt",
    "outputId": "55023945-f650-48c5-bcc7-bca108b07647"
   },
   "outputs": [],
   "source": [
    "merged_data.selectExpr(\"id\",\"explode(arrays_zip(ner_chunk.begin,ner_chunk.end,ner_chunk.result, ner_chunk.metadata)) as a\")\\\n",
    ".selectExpr(\"id\",\"a['0'] as begin\",\"a['1'] as end\",\"a['2'] as ner_chunk\",\"a['3'].entity as entity\")\\\n",
    ".orderBy(\"id\",\"begin\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "bYlwmyDhfwNa",
    "outputId": "2fe45589-e877-4924-a2a8-2f37998aa70c"
   },
   "outputs": [],
   "source": [
    "merged_data.selectExpr(\"id\",\"explode(arrays_zip(canner_chunk.begin,canner_chunk.end,canner_chunk.result, canner_chunk.metadata)) as a\")\\\n",
    ".selectExpr(\"id\",\"a['0'] as begin\",\"a['1'] as end\",\"a['2'] as ner_chunk\",\"a['3'].entity as entity\")\\\n",
    ".orderBy(\"id\",\"begin\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "-1RFnel-WEz7",
    "outputId": "0e6d5e89-b18c-4d51-c4f8-18f26b0729fd"
   },
   "outputs": [],
   "source": [
    "merged_data.selectExpr(\"id\",\"explode(arrays_zip(rex.begin,rex.end,rex.result, rex.metadata)) as a\")\\\n",
    ".selectExpr(\"id\",\"a['0'] as begin\",\"a['1'] as end\",\"a['2'] as ner_chunk\",\"a['3'].identifier as entity\")\\\n",
    ".orderBy(\"id\",\"begin\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "dpLba4tAbPiW",
    "outputId": "470f59b0-fdbe-4a05-b468-d6c9272bd2ef"
   },
   "outputs": [],
   "source": [
    "merged_data.selectExpr(\"id\",\"explode(arrays_zip(combined.result, combined.metadata)) as a\")\\\n",
    ".selectExpr(\"id\",\"a['0'] as chunk\",\"a['1'].entity as entity\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtaZ8EBBWE0B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ChunkMergeSample.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jsl250",
   "language": "python",
   "name": "jsl250"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
