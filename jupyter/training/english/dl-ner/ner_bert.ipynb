{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"colab":{"name":"ner_bert.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"MI3at4LA4TO4","colab_type":"text"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_bert.ipynb)\n","\n","## 0. Colab Setup"]},{"cell_type":"code","metadata":{"id":"CQkc9O5V6vJ5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"d42efaca-25e4-4f13-ddd1-2a5a2a51c806","executionInfo":{"status":"ok","timestamp":1589310755175,"user_tz":-120,"elapsed":21023,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}}},"source":["import os\n","\n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! java -version\n","\n","# Install pyspark\n","! pip install --ignore-installed -q pyspark==2.4.4\n","\n","# Install Spark NLP\n","! pip install --ignore-installed -q spark-nlp==2.4.5"],"execution_count":8,"outputs":[{"output_type":"stream","text":["openjdk version \"1.8.0_252\"\n","OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n","OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OnbkiY634TO7","colab_type":"text"},"source":["## Deep Learning NER\n","\n","In the following example, we walk-through a LSTM NER model training and prediction. This annotator is implemented on top of TensorFlow.\n","\n","This annotator will take a series of word embedding vectors, training CoNLL dataset, plus a validation dataset. We include our own predefined Tensorflow Graphs, but it will train all layers during fit() stage.\n","\n","DL NER will compute several layers of BI-LSTM in order to auto generate entity extraction, and it will leverage batch-based distributed calls to native TensorFlow libraries during prediction. "]},{"cell_type":"markdown","metadata":{"id":"P5NoZwVw4TO8","colab_type":"text"},"source":["#### 1. Call necessary imports and set the resource folder path."]},{"cell_type":"code","metadata":{"id":"HVMuFdHz4TO-","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","sys.path.append('../../')\n","\n","from pyspark.sql import SparkSession\n","from pyspark.ml import Pipeline\n","\n","from sparknlp.annotator import *\n","from sparknlp.common import *\n","from sparknlp.base import *\n","\n","import time\n","import zipfile\n","#Setting location of resource Directory\n","resource_path= \"../../../src/test/resources/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dnkqe7Db4TPG","colab_type":"text"},"source":["#### 2. Download CoNLL 2003 data if not present"]},{"cell_type":"code","metadata":{"id":"DtNyZXDc4TPH","colab_type":"code","colab":{}},"source":["# Download CoNLL 2003 Dataset\n","import os\n","from pathlib import Path\n","import urllib.request\n","url = \"https://github.com/patverga/torch-ner-nlp-from-scratch/raw/master/data/conll2003/\"\n","file_train=\"eng.train\"\n","file_testa= \"eng.testa\"\n","file_testb= \"eng.testb\"\n","# https://github.com/patverga/torch-ner-nlp-from-scratch/tree/master/data/conll2003\n","if not Path(file_train).is_file():   \n","    print(\"Downloading \"+file_train)\n","    urllib.request.urlretrieve(url+file_train, file_train)\n","if not Path(file_testa).is_file():\n","    print(\"Downloading \"+file_testa)\n","    urllib.request.urlretrieve(url+file_testa, file_testa)\n","\n","if not Path(file_testb).is_file():\n","    print(\"Downloading \"+file_testb)\n","    urllib.request.urlretrieve(url+file_testb, file_testb)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vmA0JH44TPP","colab_type":"text"},"source":["#### 3. Create the spark session"]},{"cell_type":"code","metadata":{"id":"O3wvVq-14TPQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"d0a3ac91-00b4-4895-8c1f-fe3f487b39ac","executionInfo":{"status":"ok","timestamp":1589310755178,"user_tz":-120,"elapsed":20993,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}}},"source":["import sparknlp \n","\n","spark = sparknlp.start()\n","\n","print(\"Spark NLP version: \", sparknlp.version())\n","print(\"Apache Spark version: \", spark.version)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Spark NLP version:  2.4.5\n","Apache Spark version:  2.4.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fxv7jokO4TPY","colab_type":"text"},"source":["#### 4. Load dataset and cache into memory"]},{"cell_type":"code","metadata":{"id":"xeuwKgWB4TPZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":459},"outputId":"926e8a4c-77a8-4a0d-a777-8f936a92a516","executionInfo":{"status":"ok","timestamp":1589310762460,"user_tz":-120,"elapsed":28266,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}}},"source":["from sparknlp.training import CoNLL\n","training_data = CoNLL().readDataset(spark, './eng.train')\n","training_data.show()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|                text|            document|            sentence|               token|                 pos|               label|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|EU rejects German...|[[document, 0, 47...|[[document, 0, 47...|[[token, 0, 1, EU...|[[pos, 0, 1, NNP,...|[[named_entity, 0...|\n","|     Peter Blackburn|[[document, 0, 14...|[[document, 0, 14...|[[token, 0, 4, Pe...|[[pos, 0, 4, NNP,...|[[named_entity, 0...|\n","| BRUSSELS 1996-08-22|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 7, BR...|[[pos, 0, 7, NNP,...|[[named_entity, 0...|\n","|The European Comm...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n","|Germany 's repres...|[[document, 0, 21...|[[document, 0, 21...|[[token, 0, 6, Ge...|[[pos, 0, 6, NNP,...|[[named_entity, 0...|\n","|\" We do n't suppo...|[[document, 0, 16...|[[document, 0, 16...|[[token, 0, 0, \",...|[[pos, 0, 0, \", [...|[[named_entity, 0...|\n","|He said further s...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 1, He...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n","|He said a proposa...|[[document, 0, 22...|[[document, 0, 22...|[[token, 0, 1, He...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n","|Fischler proposed...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 7, Fi...|[[pos, 0, 7, JJR,...|[[named_entity, 0...|\n","|But Fischler agre...|[[document, 0, 21...|[[document, 0, 21...|[[token, 0, 2, Bu...|[[pos, 0, 2, CC, ...|[[named_entity, 0...|\n","|Spanish Farm Mini...|[[document, 0, 16...|[[document, 0, 16...|[[token, 0, 6, Sp...|[[pos, 0, 6, NNP,...|[[named_entity, 0...|\n","|                   .|[[document, 0, 0,...|[[document, 0, 0,...|[[token, 0, 0, .,...|[[pos, 0, 0, ., [...|[[named_entity, 0...|\n","|Only France and B...|[[document, 0, 52...|[[document, 0, 52...|[[token, 0, 3, On...|[[pos, 0, 3, RB, ...|[[named_entity, 0...|\n","|The EU 's scienti...|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n","|Sheep have long b...|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 4, Sh...|[[pos, 0, 4, NNP,...|[[named_entity, 0...|\n","|British farmers d...|[[document, 0, 21...|[[document, 0, 21...|[[token, 0, 6, Br...|[[pos, 0, 6, JJ, ...|[[named_entity, 0...|\n","|\" What we have to...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 0, \",...|[[pos, 0, 0, \", [...|[[named_entity, 0...|\n","|Bonn has led effo...|[[document, 0, 21...|[[document, 0, 21...|[[token, 0, 3, Bo...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|\n","|Germany imported ...|[[document, 0, 84...|[[document, 0, 84...|[[token, 0, 6, Ge...|[[pos, 0, 6, NNP,...|[[named_entity, 0...|\n","|It brought in 4,2...|[[document, 0, 82...|[[document, 0, 82...|[[token, 0, 1, It...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4BO6oz8i4TPh","colab_type":"text"},"source":["#### 5. Create annotator components with appropriate params and in the right order. The finisher will output only NER. Put everything in Pipeline"]},{"cell_type":"code","metadata":{"id":"nxArxJq_4TPj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"7e8d7b5c-8f50-4913-a5a5-ccff19bc7936","executionInfo":{"status":"ok","timestamp":1589310765333,"user_tz":-120,"elapsed":31130,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}}},"source":["bert = BertEmbeddings.pretrained() \\\n"," .setInputCols([\"sentence\"])\\\n"," .setOutputCol(\"bert\")\\\n"," .setCaseSensitive(False)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["bert_base_cased download started this may take some time.\n","Approximate size to download 389.2 MB\n","[OK!]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cP9nXTCl4TPq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":938},"outputId":"6129f839-0988-4dd8-e678-1a0271f2e553","executionInfo":{"status":"error","timestamp":1589310765343,"user_tz":-120,"elapsed":30951,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}}},"source":["%%time\n","\n","# WARNING: This STEP is slow and might crash your system -- High end hardware and/or GPU required\n","## dataframe.cache() does not solve this. Results must be serialized to disk for maximum efficiency\n","### You might need to restart your driver after this step finishes\n","\n","from pathlib import Path\n","\n","with_bert_path = \"./with_bert.parquet\"\n","\n","if not Path(with_bert_path).is_dir():\n","    bert.transform(training_data).write.parquet(\"./with_bert.parquet\")\n","\n","training_with_bert = spark.read.parquet(\"./with_bert.parquet\").cache()\n","\n","print(training_with_bert.count())\n","training_with_bert.select(\"token\", \"bert\").show()"],"execution_count":14,"outputs":[{"output_type":"error","ename":"IllegalArgumentException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o161.transform.\n: java.lang.IllegalArgumentException: requirement failed: Wrong or missing inputCols annotators in BERT_EMBEDDINGS_abf30dcdf344.\n\nCurrent inputCols: sentence. Dataset's columns:\n(column_name=text,is_nlp_annotator=false)\n(column_name=document,is_nlp_annotator=true,type=document)\n(column_name=sentence,is_nlp_annotator=true,type=document)\n(column_name=token,is_nlp_annotator=true,type=token)\n(column_name=pos,is_nlp_annotator=true,type=pos)\n(column_name=label,is_nlp_annotator=true,type=named_entity).\nMake sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document, token\n\tat scala.Predef$.require(Predef.scala:224)\n\tat com.johnsnowlabs.nlp.AnnotatorModel._transform(AnnotatorModel.scala:43)\n\tat com.johnsnowlabs.nlp.AnnotatorModel.transform(AnnotatorModel.scala:79)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-bba92a66dc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n# WARNING: This STEP is slow and might crash your system -- High end hardware and/or GPU required\\n## dataframe.cache() does not solve this. Results must be serialized to disk for maximum efficiency\\n### You might need to restart your driver after this step finishes\\n\\nfrom pathlib import Path\\n\\nwith_bert_path = \"./with_bert.parquet\"\\n\\nif not Path(with_bert_path).is_dir():\\n    bert.transform(training_data).write.parquet(\"./with_bert.parquet\")\\n\\ntraining_with_bert = spark.read.parquet(\"./with_bert.parquet\").cache()\\n\\nprint(training_with_bert.count())\\ntraining_with_bert.select(\"token\", \"bert\").show()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIllegalArgumentException\u001b[0m: \"requirement failed: Wrong or missing inputCols annotators in BERT_EMBEDDINGS_abf30dcdf344.\\n\\nCurrent inputCols: sentence. Dataset's columns:\\n(column_name=text,is_nlp_annotator=false)\\n(column_name=document,is_nlp_annotator=true,type=document)\\n(column_name=sentence,is_nlp_annotator=true,type=document)\\n(column_name=token,is_nlp_annotator=true,type=token)\\n(column_name=pos,is_nlp_annotator=true,type=pos)\\n(column_name=label,is_nlp_annotator=true,type=named_entity).\\nMake sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document, token\""]}]},{"cell_type":"code","metadata":{"id":"ewZNMRkX4TPz","colab_type":"code","colab":{}},"source":["nerTagger = NerDLApproach()\\\n","  .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n","  .setLabelColumn(\"label\")\\\n","  .setOutputCol(\"ner\")\\\n","  .setMaxEpochs(1)\\\n","  .setRandomSeed(0)\\\n","  .setVerbose(0)\n","\n","converter = NerConverter()\\\n","  .setInputCols([\"document\", \"token\", \"ner\"])\\\n","  .setOutputCol(\"ner_span\")\n","\n","pipeline = Pipeline(\n","    stages = [\n","    nerTagger,\n","    converter\n","  ])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jmrxa0zb4TP5","colab_type":"text"},"source":["#### 6. Train the pipeline. (This will take some time)"]},{"cell_type":"code","metadata":{"id":"M1EsnzJD4TP6","colab_type":"code","colab":{}},"source":["%%time\n","\n","start = time.time()\n","print(\"Start fitting\")\n","model = pipeline.fit(training_with_bert)\n","print(\"Fitting is ended\")\n","print (time.time() - start)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N13yqmUu4TQA","colab_type":"text"},"source":["#### 7. Lets predict with the model"]},{"cell_type":"code","metadata":{"id":"sc9NJ1EV4TQB","colab_type":"code","colab":{}},"source":["document = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","sentence = SentenceDetector()\\\n","    .setInputCols(['document'])\\\n","    .setOutputCol('sentence')\n","\n","token = Tokenizer()\\\n","    .setInputCols(['sentence'])\\\n","    .setOutputCol('token')\n","\n","prediction_pipeline = Pipeline(\n","    stages = [\n","        document,\n","        sentence,\n","        token,\n","        bert,\n","        model\n","    ]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"07EWw0mG4TQR","colab_type":"code","colab":{}},"source":["prediction_data = spark.createDataFrame([[\"Germany is a nice place\"]]).toDF(\"text\")\n","prediction_data.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"50yCGM6F4TQZ","colab_type":"code","colab":{}},"source":["prediction_model = prediction_pipeline.fit(prediction_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHk2VbE_4TQf","colab_type":"code","colab":{}},"source":["%%time\n","\n","lp = LightPipeline(prediction_model)\n","result = lp.annotate(\"International Business Machines Corporation (IBM) is an American multinational information technology company headquartered in Armonk.\")\n","for e in list(zip(result['token'], result['ner'])):\n","    print(e)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwNEGQts4TQl","colab_type":"code","colab":{}},"source":["%%time\n","\n","# This might take 8 minutes. Timing is not lineal\n","\n","prediction_model.transform(prediction_data).show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hxYs7v3F4TQq","colab_type":"text"},"source":["#### 8. Save both pipeline and single model once trained, on disk"]},{"cell_type":"code","metadata":{"id":"nDk4xWbT4TQr","colab_type":"code","colab":{}},"source":["prediction_model.write().overwrite().save(\"./ner_dl_model\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzvHfHBr4TQx","colab_type":"text"},"source":["#### 9. Load both again, deserialize from disk"]},{"cell_type":"code","metadata":{"id":"ARYxI8594TQz","colab_type":"code","colab":{}},"source":["from pyspark.ml import PipelineModel, Pipeline\n","\n","loaded_prediction_model = PipelineModel.read().load(\"./ner_dl_model\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xfVgn3ZI4TQ4","colab_type":"code","colab":{}},"source":["%%time\n","lp = LightPipeline(loaded_prediction_model)\n","result = lp.annotate(\"Peter is a good person.\")\n","for e in list(zip(result['token'], result['ner']))[:10]:\n","    print(e)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UpDPutD_4TQ-","colab_type":"code","colab":{}},"source":["for stage in loaded_prediction_model.stages:\n","    print(stage)\n","print(loaded_prediction_model.stages[-1].stages)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aH191rNe4TRC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}