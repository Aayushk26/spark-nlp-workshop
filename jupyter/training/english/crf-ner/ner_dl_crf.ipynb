{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m32x7R0tyHH6"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/crf-ner/ner_dl_crf.ipynb)\n",
    "\n",
    "## 0. Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64364,
     "status": "ok",
     "timestamp": 1589302153034,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "95EqKJCoySwe",
    "outputId": "d475c979-4a17-4430-e4e3-2d1c00e889d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 56kB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 50.7MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 112kB 4.8MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed -q spark-nlp==2.4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxBXISVpyHIA"
   },
   "source": [
    "## CRF Named Entity Recognition\n",
    "In the following example, we walk-through a Conditional Random Fields NER model training and prediction.\n",
    "\n",
    "This challenging annotator will require the user to provide either a labeled dataset during fit() stage, or use external CoNLL 2003 resources to train. It may optionally use an external word embeddings set and a list of additional entities.\n",
    "\n",
    "The CRF Annotator will also require Part-of-speech tags so we add those in the same Pipeline. Also, we could use our special RecursivePipeline, which will tell SparkNLP's NER CRF approach to use the same pipeline for tagging external resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jePQ1RvIyHIC"
   },
   "source": [
    "#### 1. Call necessary imports and set the resource path to read local data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yljiat0_yHIE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import time\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9kYUUsYyHIP"
   },
   "source": [
    "#### 2. Download training dataset if not already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66022,
     "status": "ok",
     "timestamp": 1589302154705,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "IKJQpW57yHIR",
    "outputId": "ee3edc25-59ae-4bf7-a5f9-d0902d165197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Not found will downloading it!\n"
     ]
    }
   ],
   "source": [
    "# Download CoNLL 2003 Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "if not Path(\"eng.train\").is_file():\n",
    "    print(\"File Not found will downloading it!\")\n",
    "    url = \"https://github.com/patverga/torch-ner-nlp-from-scratch/raw/master/data/conll2003/eng.train\"\n",
    "    urllib.request.urlretrieve(url, 'eng.train')\n",
    "else:\n",
    "    print(\"File already present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7ixJew3yHIc"
   },
   "source": [
    "#### 3. Load SparkSession if not already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 85487,
     "status": "ok",
     "timestamp": 1589302174178,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "0ENw-DRoyHIe",
    "outputId": "288a3563-67b9-425b-bbfd-168b132ff84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  2.4.5\n",
      "Apache Spark version:  2.4.4\n"
     ]
    }
   ],
   "source": [
    "import sparknlp \n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmJej2gGyHIp"
   },
   "source": [
    "#### 4. Create annotator components in the right order, with their training Params. Finisher will output only NER. Put all in pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uKWI52qyHIs"
   },
   "outputs": [],
   "source": [
    "nerTagger = NerCrfApproach()\\\n",
    "  .setInputCols([\"sentence\", \"token\", \"pos\", \"embeddings\"])\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setOutputCol(\"ner\")\\\n",
    "  .setMinEpochs(1)\\\n",
    "  .setMaxEpochs(1)\\\n",
    "  .setLossEps(1e-3)\\\n",
    "  .setL2(1)\\\n",
    "  .setC0(1250000)\\\n",
    "  .setRandomSeed(0)\\\n",
    "  .setVerbose(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVRUX_hpyHIz"
   },
   "source": [
    "#### 6. Load a dataset for prediction. Training is not relevant from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115939,
     "status": "ok",
     "timestamp": 1589302204641,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "edIbRJiNyHI1",
    "outputId": "46a37ded-aea9-45c2-d56e-51b26c0ba2e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|          embeddings|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|EU rejects German...|[[document, 0, 47...|[[document, 0, 47...|[[token, 0, 1, EU...|[[pos, 0, 1, NNP,...|[[named_entity, 0...|[[word_embeddings...|\n",
      "|     Peter Blackburn|[[document, 0, 14...|[[document, 0, 14...|[[token, 0, 4, Pe...|[[pos, 0, 4, NNP,...|[[named_entity, 0...|[[word_embeddings...|\n",
      "| BRUSSELS 1996-08-22|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 7, BR...|[[pos, 0, 7, NNP,...|[[named_entity, 0...|[[word_embeddings...|\n",
      "|The European Comm...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|[[word_embeddings...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "conll = CoNLL()\n",
    "data = conll.readDataset(spark, path='eng.train')\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained()\\\n",
    ".setOutputCol('embeddings')\n",
    "\n",
    "ready_data = embeddings.transform(data)\n",
    "\n",
    "ready_data.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUPzqnGqyHI-"
   },
   "source": [
    "#### 7. Training the model. Training doesn't really do anything from the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 401300,
     "status": "ok",
     "timestamp": 1589302490013,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "7bCbmaLDyHI_",
    "outputId": "bfa220ce-aeb4-42b8-8584-87b273a04d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting\n",
      "Fitting has ended\n",
      "285.30734848976135\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Start fitting\")\n",
    "ner_model = nerTagger.fit(ready_data)\n",
    "print(\"Fitting has ended\")\n",
    "print (time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W7vA5MiOyHJH"
   },
   "source": [
    "#### 8. Save NerCrfModel into disk after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_LY10D9yHJJ"
   },
   "outputs": [],
   "source": [
    "ner_model.write().overwrite().save(\"./pip_wo_embedd/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6qtW2x5yHJQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "ner_dl_crf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
